Hi, this is Eric Johnson. It's February 18, 2021. And this is the Engineering Key Review at GitLab. So I've got number four in the agenda, which is a proposal to break up this meeting into four department key reviews. So currently, this is engineering development quality secure D and UX. Infrastructure and Support do their own key reviews. Already, I have the reasons why increased visibility able to go deeper, increase the objectivity with which my reports can manage their groups, allow me more time to focus on new markets, and allow me to shift into more of a question asker mode than generating content and answering questions in these meetings. But to avoid adding three net new meetings to stakeholders calendars, I propose we do a sort of two month rotation. So Month One development quality go. Month, two security and UX would go how do people feel about that proposal? I think in the group conversations, it's working really well, so I'm supportive. And this is the smallest thing. Maybe we need four meetings a month, like it's the biggest apartment. It's super essential. But you propose this. I could see either way, so let's stick with the proposal. Cool. We'll try it, and we can be flexible. I mean, development is larger. Maybe they go more frequently or something like that, but we'll see how it goes. All right. And then I've got number five, which is we've got R and D overall Mr rate, and we also have R and D wider Mr rate, both as top level KPIs for engineering. So the difference between them in the simplest sense is that R and D, wider Mr rate includes both community contributions and community Mrs. The problems I see with this are that one, the wider Mr rate, the one that includes internal and external Mrs, it duplicates the overall Mr rate, which is the wider Mr rate should just be external. Right. And then overall should be narrow plus wider, like we say, the wider community. Right. Okay. I have to check the taxonomy. Lily, can you confirm Sid's reasoning is my understanding as well? Yeah, I believe Water Mr rate just captures community contributions only and no internal. Yeah. And the reason we measure that is that one of the most likely failure modes is that we lose the community. Yeah. Eric, where it gets goofy is that when you look at a specific team within the company, there could be contributions outside of that that aren't community contributions. They would be viewed community contributions by that group, but effectively, they're not from outside the company. So that's why we use wider to kind of reflect that. And narrow is very specific to the team. Are you saying that if someone in plan contributes to verify, it's viewed as wider? Not quite. That plan and verify are just fine. It's when you look at the development versus infrastructure, infrastructure will oftentimes contribute to development's work, but it won't be counted as Mrs. Okay. That's a potential bit of funkiness that we should talk about separately. I didn't have that in my sort of critique of this, but that doesn't necessarily make intuitive sense to me. So then I think part of my critique of this can be thrown out because it's not as duplicative as I thought. But I still think there's a problem with R and D wider Mr rate, which is this thing doesn't really move in part because it's a rate. So it feels like the way to drive this up is to specifically drive community authors to contribute more than one Mr per month. That's how this moves up because it's a productivity rate like we use internally. And that doesn't necessarily feel like the right thing because there are scenarios in which this goes up. We've actually got less contributions overall and less contributors overall. Wait a second. So you're saying that R and D wider Mr rate is number Mrs per external contributor. Oh my goodness. That should not be the thing. It should be contributions per GitLab team member. The thing above the division is the external ones. The thing below is the number of team members at GitLab. Is that the case, Lily? I'm checking right now. I think so in our new March merge per team member. So unless we start calling people outside the company team member, then it shouldn't be done. Yeah, just clarifying here. So our numerator is community contributions and then our denominator is GitLab team members. So it's not per external member. What we're doing there, Eric, is we're not trying to say how many Mrs does someone send if they send something, we're saying how many Mrs from external do we get for the size of our organization? Sorry, have a childhood emergency outside the door. Maybe explain the context behind this. The context is as we grow as a company, we should make sure we keep the community up. Like the logical thing is for the community to flatline and the size of the ORC to go, and before you know it, you've kind of outgrown the wider community. Yeah. What I'm seeing is we created this pretty sophisticated taxonomy with prefixes and post fixes to talk about these things, but in reality we've only got two of them and we keep forgetting and we have a hard time discussing this thing. So I'd rather just name them simply two names for what they are rather than using the taxonomy. But also in F, I have this proposal of like, what if we just tracked as a KPI the percentage of total Mrs that come from the community over time and we would see that drop. I love that. Let's do that instead. Okay, but the thing why we have this complex thing is because you can game that you want to game that. You just produce fewer Mrs with the engineers at GitLab. So if you drive that really hard and say, this is your number one goal. It's very easy to achieve. You just tell all your engineers to produce half. Yeah. So we have different metrics to prevent that from happening. The same way that support SLAs and SSAC kind of buttress one another. I think we're robust to that. But simplifying, this would make these conversations go if you, as our CTO, don't even understand them, we went overboard. So I'm supportive understand them. And I forgot and I was reviewing the stuff this morning, I'm like, there's a problem with this. And then you just remind me of the context. So, yeah, if I can't hold it my head percentage that come from the community. I love that. It's what all our investors ask about. Let's do that. Okay, cool. So, Lily, if you can work with Mac to make that transition, that would be great. And I'll bold the one that we're talking about. That's Roman numulsery. Thanks. I'm on the call. Sorry it was a bit late. Timeline. We do have PiS on the raw number of community Mrs and we can make the shift and why they're confirming from the definition. I think wider only counts for community, and that's what the definition is. Cool. All right, so number six then. Christopher? Sorry, I was looking up to see if I had the percentage graph because I think we played around with this at one point and had a draft of that probably about five months back, if I can remember. Lily, just an FYI, the month of February. If you were looking at any particular metrics, particularly in development at an Mr rate, we haven't had updates in four days. There's apparently a lag issue that's been problematic for the data team to basically get updated metrics. And they're working on that. I'm sorry, Meck, you got the next one. So, yeah, there's some color there on mitigation the lag later on to seven, I said FYI in addition to the KDPI status. I'm sorry, Mac. I wanted to just touch on the postgres replication issue there real quick. I've been trying to get my arms wrapped around it. Do we have the right attention to this? This is kind of eric, I don't know if you were commenting on hinting towards this in the last meeting around some of the infrastructure improvements on the product side. I'm just not quite sure whose responsibility it is to focus on getting a handle on some of the constraints we have on replication. Yeah, what I was mentioning in the product you review about an hour ago is I think is sort of like unrelated. And so I think the DRI needs to be your kind of data engineering team. But of course, there's a dependency on infrastructure because that's where the data is being piped from. They do own that data source. Yeah. I'll say for the replication lag on that slave host where I'm sorry, not on the secondary host where the data is being pulled from the infrastructure would be the DRI for that. And so any escalations, but I'll own those and I know we have an action plan for that as far as creating another dedicated host just for the data team to pull from. Okay, I saw that issue and I did talk to Craig gomes a little bit as well on the database side just to see if there's some database improvements. And I'm still trying to figure out if it's truly just dedicated computational sort of resource, a server, or if there's actually some database tuning that needs to occur. Do you have a sense of that? I'd say it's three different things. It's having a dedicated host that doesn't have conflicting query traffic coming from other workloads. There are some tuning improvements to be made and then there's also improvements in and this is where it does maybe relate a little bit to what the topic was in the last review. Basically the overall demand on the database layer from.com activities and improving those. So it's definitely not just one of those things. But one of the most specific actions we're going to take, though is separating out and having a dedicated host so that we're just dealing with the profile of the data engineering traffic on there and not having conflicting queries affect the ability to update the replication. Steve, I definitely want to partner with you on this one because I think the demand on those databases is only going to increase, it's not going down. And I think we need to get I'm still unclear on where to focus in to get the biggest bang for the buck. I think of the computational resource dedication, that's going to be a good thing, but we're probably going to squeeze the balloon and then the next area will unearth itself. Okay, I'll tell you what, I'll put into the infra key review for next week, an update on this issue. Thank you, Steve. So then back to MEC on seven or yes, thank you. And Rob was in the assassin this morning as well. Brian, we have the attention there. Number seven. Just to provide an update on previous conversations, we're continuing to improve defect tracking and against SLOs. There is a first iteration pi that we are experimenting to show percentage of defects meeting the SLOs key findings. S ones are hovering at 80%, s two at 60. We've been focused mostly on s one and S two S at this point, as hence why s three and s four S are lower, and this will likely be the case. We are also in point B, are working on the measurement for average open bugs age. This would give us a whole picture of what's left if the age goes up or down. If we are cleaning the backlog, the average age should go down as well. There's no pi yet, but I just want to update and ensure beyond this it's not off track number C. Craig on s Two. Yeah, just I was looking through the charts and I noted that there was a spike in meantime to close and just wanted to see if you had any insight into that for us. This is the s two S. The s one looked fine. Yeah, this is where the point b on H supplemental charts in the back end helps. So I haven't seen a dip in age nor the count overall. I think it's the latter. We need to dig in a bit deeper in that and also the data lag. I would like to reevaluate when we have the whole picture, when everything is synced in as well. Christy, you have some insights? Yeah, I'm just wondering if part of this could be the fact that we changed the severity across the board for Mrs to S Two and so we may have some older bugs in there that hadn't been addressed because they were at a lower severity. Now we've moved them to S Two and maybe that caused a little spike. That could be the case. If we did it in a limited fashion, it won't be a huge volume. We also iterated after that to pin on priority since product owns prioritization, so I wouldn't account it entirely to that. This isn't the infra key review, but I know that they've gotten backed up on those issues. So if some good portion of those are infra created or related then that might be lifting it as well. I can take the deeper dig in and then provide an update next time. I think we need extra debug slicing of the data here. Did you like to go to .8? Yeah. We are now measuring S One S Two slo achievement with closed bugs. But if you then look at the number of bugs it's exponential growth and then it would be trivial to achieve 100% slo achievement. If you just look at closed bugs, even though there would be a major problem in the company, 99% of all bugs are overdue. As long as I only close ones that are still within the slo, I'll have great achievement. So I think we shouldn't be looking at the closed bugs, I think we should be looking at open bugs, the entire population, what percentage of dose is within the slo time. I think we're doing it the wrong way. Thanks for the feedback. Hence why we wanted to have the average age to measure what's outside in the open. We can make this iteration to also measure focus on the age of all opened, including open bugs. This is also something I have discussed with Christopher in the next iteration as well. And we're happy, more than happy to adjust. Go ahead. So average age would get closer to it. It's not what I'm proposing. What I'm proposing is of the open bugs, what percentage is outside of slo. So display it as a percentage you do now, just do it about the open bugs, not the closed one. Got it. Okay. The exceeding Slo for open bugs. Yeah, or open bugs that are within Slo. So you have a chart that should go up and to the right like everything else. Sounds great. We can take it to the next data metrics work stream to deliver this. Cool. Thanks. A little tricky, Mac. You'll have to figure out how to because we like to be able to have charts that we can historically reconstruct if we need to. So when tickets close out, you need to go through their history to figure out at this time when it was open, did it breach the Slo or not? That's a good point. This might be much harder computationally, so I totally respect if we can do it for that reason. Cool. Nine. Craig? Yeah. I just wanted to ask the team, like, I went through all the key meeting metrics. Everything looked in line with prior periods and looked good. Is there anything the team wants to call out supposedly that we should be watching? Yeah, I'll call out Sus. So the good news is, in Q Four, we had our smallest decline over several quarters, so we only went down by a 10th of a point. The quarter previous was 0.6 or six tenths of a point, and the quarter before that was a full point. So we see this as an improvement, even though it was still a decline. But it's still a decline. Obviously, we want this actually tracking in an upward direction. We also don't have enough data to know whether or not this is an actual real trend up, so I'm optimistic. I think this is a good thing. We have had a much keener focus on Sus over the past several quarters. So that's why I think okay. The work that we've done, I think, actually is catching up and getting noticed in Sus, but we got to keep an eye on it. We cannot assume that that's the case. Yeah. And the bug discussion above just kind of points out that we have an underlying problem right now in our metrics measurement. So if we change the measurements to reflect that, then hopefully we're in good shape. If we don't and we flatline and address it so that we flatline open S ones and S two S, you will see a temporary jump above SLOs as we clear out that backlog over that period of time. And I have point C, which is similar to infrastructure. We need to get more security work prioritized or hearing that from the team. But neither that problem nor that activity is sort of currently reflected in our security metrics. So we have some work to do long term to make sure that we see things like that in the metrics and the measurements that we're making. So back to you, Sid Ten. Yeah. The narrow Mr rate seems significantly below target, and maybe I hope that it would bounce back from December. I think it bounced back but not back on target. Any context there? What's going on? Yeah. So with family and friends days, we actually had some heavier vacation days in January than we historically have. One thing to note is that we are actually at a higher Mr rate. If you go back the last 18 months, we're actually at a higher narrow Mr rate than we were back in each month this year. So if you compare October to October, November to November, november, December and January comparatively to last year, what you'll find is we're between a half point and a 1.5 Mr rate above where we were in the month of previous year. That's great context. Thank you, Christopher. Good work. Yeah. So the expectation is that February is a short month. We are at, I think, 16 work days with friends and family day and other things. Obviously, us having power outages in Texas doesn't help things either for the folks who are working in Texas, but hopefully the rest of the team is being effective. I was hoping to see a better result right now, but with four or five days, particularly around release week, that's usually when we do see a little bit higher activity. So that's not accounted for yet. But March is when I'm expecting to kind of see a real rebound, much like we did last year. Awesome. Thanks. The other context I'll give is we now do time series targets, so when we change the target, you'll see that reflected in the line. So if we were to look back historically here, the goal here was actually lower, and Christopher was ambitious and we kept raising it. We kept meeting that. So it should stairstep here and we could go back and reconstruct that if we really wanted to. And then ignoring the sort of the seasonal dip here, we raised it to, I think, eleven. And then we realized we're kind of hitting that point of diminishing returns. And the right thing to do business wise, and this is in our FY 22 direction, is hold the line at productivity, but start to raise other things related to quality, security, availability and whatnot. So that's kind of why you're seeing this bump as we raised it and we realized, okay, we shouldn't raise it anymore, and we brought it back down to ten. So ten will be the target going forward. I'm going to try to get better at a lot of other things while preventing this from dipping. Cool. And I just want to call out that it's not necessarily higher Mr rates should also help to address security and quality and other things because you're more productive, so you can fix more things. So it's not necessarily opposite, but I agree with let's hold the line. Ten is ten is a great number and focus on other indicators to improve. That makes a ton of sense. Cool. Well said. All right, that's it. For the agenda. Anyone want to vocalize anything else? Great. Well, thanks, everybody. And I'm going to go check on my four year old and see if she got whatever she needed. So cheers and talk soon. Thanks, Sarah.